# import streamlit as st
# import cv2
# from ultralytics import YOLO
# import numpy as np
# from PIL import Image

# # í˜ì´ì§€ ì„¤ì •
# st.set_page_config(page_title="YOLO Model Demo", layout="centered")

# # í˜ì´ì§€ ì œëª© ë° ì„¤ëª…
# st.title("ğŸ¤¸â€â™€ï¸YOLOv8")
# st.title("RockâœŠ, ScissorsâœŒ, PaperğŸ– Detection")
# st.write("""
# YOLOv8 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°€ìœ„, ë°”ìœ„, ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
# ì´ ëª¨ë¸ì€ Roboflowë¥¼ í†µí•´ í•™ìŠµëœ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ë©°, ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤:
# - ë°ì´í„° ìˆ˜ì§‘ ë° ë¼ë²¨ë§
# - ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦
# - ì›¹ìº ì„ í†µí•œ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€
# """)

# # ìˆ˜í‰ìœ¼ë¡œ ì´ë¯¸ì§€ ì •ë ¬
# col1, col2 = st.columns(2)

# # ê° ì»¬ëŸ¼ì— ì´ë¯¸ì§€ ì¶”ê°€
# with col1:
#     st.image("https://github.com/user-attachments/assets/cbc639f2-7055-419e-a94e-6e1fbe143b61", 
#              caption="Rock, Paper, Scissors Dataset", use_column_width=True)

# with col2:
#     st.image("https://github.com/user-attachments/assets/3840f1b6-06ce-401b-b242-1dc0e1dbf891", 
#              caption="YOLOv8 Model", use_column_width=True)

# # ëª¨ë¸ ì„¤ëª… ì¶”ê°€
# st.write("""
# ìœ„ì˜ ì´ë¯¸ì§€ëŠ” í•™ìŠµëœ ë°ì´í„°ì…‹ê³¼ YOLOv8 ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
# ì‹¤ì‹œê°„ ì›¹ìº ì—ì„œ ê°€ìœ„âœŒ, ë°”ìœ„âœŠ, ë³´ğŸ– ì¤‘ í•˜ë‚˜ë¥¼ ë‚´ë©´
# yolov8 ëª¨ë¸ì„ í†µí•´ ë¶„ë¥˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ !âœ¨
# """)

# # ì›¹ìº  ì„ íƒ ë° ì„¤ì •
# st.header("ì›¹ìº ì„ í†µí•´ ì‹¤ì‹œê°„ ë¶„ë¥˜í•˜ê¸°")
# camera_index = st.sidebar.selectbox("Select Camera Index", [0, 1, 2])

# # # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
# model = YOLO('./custom_train/yolov8n_rock_paper_scissors.pt')  # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ

# # ëª¨ë¸ í´ë˜ìŠ¤ ì´ë¦„ ì¶œë ¥
# st.sidebar.text("Model Classes:")
# # st.sidebar.write(model.names)

# # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ ë¶„ë¥˜ ì„¹ì…˜
# st.sidebar.header("Image Upload for Classification")
# uploaded_file = st.sidebar.file_uploader("Upload an image", type=["jpg", "jpeg", "png"])

# if uploaded_file is not None:
#     # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ PILë¡œ ì—´ê¸°
#     image = Image.open(uploaded_file)

#     # ì´ë¯¸ì§€ë¥¼ OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
#     frame = np.array(image)
#     frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

#     # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ í‘œì‹œ
#     st.image(image, caption="Uploaded Image", use_column_width=True)

#     # ê°ì²´ íƒì§€ (Rock, Paper, Scissors í´ë˜ìŠ¤ íƒì§€)
#     results = model.predict(frame, classes=[0, 1, 2], conf=0.4, imgsz=640)

#     # íƒì§€ëœ ê²°ê³¼ ì‹œê°í™”
#     annotated_frame = results[0].plot()

#     # BGR ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜ (OpenCVëŠ” BGR í˜•ì‹ì´ë¯€ë¡œ, RGB í˜•ì‹ìœ¼ë¡œ ë³€í™˜ í•„ìš”)
#     annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)

#     # Streamlitì„ í†µí•´ íƒì§€ëœ ì´ë¯¸ì§€ í‘œì‹œ
#     st.image(annotated_frame, caption="Detected Image", use_column_width=True)

#     # ê°ì§€ëœ ê°ì²´ ëª©ë¡ í‘œì‹œ
#     st.subheader("Detected Objects")
#     if len(results[0].boxes) > 0:
#         for box in results[0].boxes:
#             cls = int(box.cls[0])
#             label = model.names[cls]
#             confidence = box.conf[0]
#             st.write(f"Detected {label} with {confidence:.2f} confidence.")
#     else:
#         st.write("No objects detected.")

# # ì›¹ìº  ìŠ¤íŠ¸ë¦¬ë°
# st.header("Real-Time Classification with Webcam")

# # ì›¹ìº  ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ë° ì¤‘ì§€ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” ë³€ìˆ˜
# if "streaming" not in st.session_state:
#     st.session_state.streaming = False

# # ì›¹ìº  ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ë²„íŠ¼
# start_button = st.button("Start Webcam")
# if start_button:
#     st.session_state.streaming = True

# # ì›¹ìº  ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€ ë²„íŠ¼
# stop_button = st.button("Stop Webcam")
# if stop_button:
#     st.session_state.streaming = False

# # ì›¹ìº  ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœì— ë”°ë¥¸ ì²˜ë¦¬
# if st.session_state.streaming:
    
#     # solutions.inference(model="./custom_train/yolov8n_rock_paper_scissors.pt")
#     stframe = st.empty()  # Streamlitì—ì„œ ì‚¬ìš©í•  ë¹ˆ ì´ë¯¸ì§€ í”„ë ˆì„ ì„¤ì •

#     # while st.session_state.streaming:
#     # st.camera_input()ì„ ì‚¬ìš©í•˜ì—¬ ì›¹ìº ì—ì„œ í”„ë ˆì„ ìº¡ì²˜
#     img_file_buffer = st.camera_input("Capture")

#     if img_file_buffer is not None:
#         # ì´ë¯¸ì§€ë¥¼ OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
#         bytes_data = img_file_buffer.getvalue()
#         frame = cv2.imdecode(np.frombuffer(bytes_data, np.uint8), cv2.IMREAD_COLOR)
#         # frame = img_file_buffer.getvalue()
#         # frame = cv2.imdecode(np.frombuffer(frame, np.uint8), cv2.IMREAD_COLOR)
#         # frame = np.array(Image.open(img_file_buffer))
#         # frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

#         # ê°ì²´ íƒì§€ (Rock, Paper, Scissors í´ë˜ìŠ¤ íƒì§€)
#         results = model.predict(frame, classes=[0, 1, 2], conf=0.4, imgsz=640)

#         # íƒì§€ëœ ê²°ê³¼ ì‹œê°í™”
#         annotated_frame = results[0].plot()

#         # BGR ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜ (OpenCVëŠ” BGR í˜•ì‹ì´ë¯€ë¡œ, RGB í˜•ì‹ìœ¼ë¡œ ë³€í™˜ í•„ìš”)
#         annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)

#         # Streamlitì„ í†µí•´ ì´ë¯¸ì§€ í‘œì‹œ
#         stframe.image(annotated_frame, channels="RGB")

#     st.write("Webcam streaming stopped.")



import logging
import queue
from typing import List, NamedTuple

import av
import cv2
import numpy as np
import streamlit as st
from streamlit_webrtc import WebRtcMode, webrtc_streamer

from ultralytics import YOLO

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="YOLO Model Demo", layout="centered")

# í˜ì´ì§€ ì œëª© ë° ì„¤ëª…
st.title("ğŸ¤¸â€â™€ï¸YOLOv8")
st.title("RockâœŠ, ScissorsâœŒ, PaperğŸ– Detection")
st.write("""
YOLOv8 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°€ìœ„, ë°”ìœ„, ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
ì´ ëª¨ë¸ì€ Roboflowë¥¼ í†µí•´ í•™ìŠµëœ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ë©°, ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤:
- ë°ì´í„° ìˆ˜ì§‘ ë° ë¼ë²¨ë§
- ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦
- ì›¹ìº ì„ í†µí•œ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€
""")

# ìˆ˜í‰ìœ¼ë¡œ ì´ë¯¸ì§€ ì •ë ¬
col1, col2 = st.columns(2)

# ê° ì»¬ëŸ¼ì— ì´ë¯¸ì§€ ì¶”ê°€
with col1:
    st.image("https://github.com/user-attachments/assets/cbc639f2-7055-419e-a94e-6e1fbe143b61", 
             caption="Rock, Paper, Scissors Dataset", use_column_width=True)

with col2:
    st.image("https://github.com/user-attachments/assets/3840f1b6-06ce-401b-b242-1dc0e1dbf891", 
             caption="YOLOv8 Model", use_column_width=True)

# ëª¨ë¸ ì„¤ëª… ì¶”ê°€
st.write("""
ìœ„ì˜ ì´ë¯¸ì§€ëŠ” í•™ìŠµëœ ë°ì´í„°ì…‹ê³¼ YOLOv8 ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
ì‹¤ì‹œê°„ ì›¹ìº ì—ì„œ ê°€ìœ„âœŒ, ë°”ìœ„âœŠ, ë³´ğŸ– ì¤‘ í•˜ë‚˜ë¥¼ ë‚´ë©´
yolov8 ëª¨ë¸ì„ í†µí•´ ë¶„ë¥˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ !âœ¨
""")

# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
model_path = './custom_train/yolov8n_rock_paper_scissors.pt'
model = YOLO(model_path)  # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ

# ëª¨ë¸ í´ë˜ìŠ¤ ì´ë¦„ ì¶œë ¥
st.sidebar.text("Model Classes:")
st.sidebar.write(model.names)

# ì—…ë¡œë“œëœ ì´ë¯¸ì§€ ë¶„ë¥˜ ì„¹ì…˜
st.sidebar.header("Image Upload for Classification")
uploaded_file = st.sidebar.file_uploader("Upload an image", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ PILë¡œ ì—´ê¸°
    image = Image.open(uploaded_file)

    # ì´ë¯¸ì§€ë¥¼ OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    frame = np.array(image)
    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

    # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ í‘œì‹œ
    st.image(image, caption="Uploaded Image", use_column_width=True)

    # ê°ì²´ íƒì§€ (Rock, Paper, Scissors í´ë˜ìŠ¤ íƒì§€)
    results = model.predict(frame, classes=[0, 1, 2], conf=0.4, imgsz=640)

    # íƒì§€ëœ ê²°ê³¼ ì‹œê°í™”
    annotated_frame = results[0].plot()

    # BGR ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜ (OpenCVëŠ” BGR í˜•ì‹ì´ë¯€ë¡œ, RGB í˜•ì‹ìœ¼ë¡œ ë³€í™˜ í•„ìš”)
    annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)

    # Streamlitì„ í†µí•´ íƒì§€ëœ ì´ë¯¸ì§€ í‘œì‹œ
    st.image(annotated_frame, caption="Detected Image", use_column_width=True)

    # ê°ì§€ëœ ê°ì²´ ëª©ë¡ í‘œì‹œ
    st.subheader("Detected Objects")
    if len(results[0].boxes) > 0:
        for box in results[0].boxes:
            cls = int(box.cls[0])
            label = model.names[cls]
            confidence = box.conf[0]
            st.write(f"Detected {label} with {confidence:.2f} confidence.")
    else:
        st.write("No objects detected.")

# ì›¹ìº  ìŠ¤íŠ¸ë¦¬ë°
st.header("Real-Time Classification with Webcam")

class Detection(NamedTuple):
    class_id: int
    label: str
    score: float
    box: np.ndarray

# NOTE: The callback will be called in another thread,
#       so use a queue here for thread-safety to pass the data
#       from inside to outside the callback.
result_queue: "queue.Queue[List[Detection]]" = queue.Queue()

def video_frame_callback(frame: av.VideoFrame) -> av.VideoFrame:
    image = frame.to_ndarray(format="bgr24")

    # ê°ì²´ íƒì§€ (Rock, Paper, Scissors í´ë˜ìŠ¤ íƒì§€)
    results = model.predict(image, classes=[0, 1, 2], conf=0.4, imgsz=640)

    h, w = image.shape[:2]

    # Convert the output array into a structured form.
    detections = [
        Detection(
            class_id=int(box.cls[0]),
            label=model.names[int(box.cls[0])],
            score=float(box.conf[0]),
            box=(box.xyxy[0] * np.array([w, h, w, h])),
        )
        for box in results[0].boxes
    ]

    # Render bounding boxes and captions
    for detection in detections:
        caption = f"{detection.label}: {round(detection.score * 100, 2)}%"
        color = (0, 255, 0)  # Green color for bounding box
        xmin, ymin, xmax, ymax = detection.box.astype("int")

        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 2)
        cv2.putText(
            image,
            caption,
            (xmin, ymin - 15 if ymin - 15 > 15 else ymin + 15),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.5,
            color,
            2,
        )

    result_queue.put(detections)

    return av.VideoFrame.from_ndarray(image, format="bgr24")

# ì›¹ìº  ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ë²„íŠ¼
start_button = st.button("Start Webcam")
if start_button:
    st.session_state.streaming = True

# ì›¹ìº  ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€ ë²„íŠ¼
stop_button = st.button("Stop Webcam")
if stop_button:
    st.session_state.streaming = False

# ì›¹ìº  ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœì— ë”°ë¥¸ ì²˜ë¦¬
if st.session_state.get("streaming", False):
    webrtc_ctx = webrtc_streamer(
        key="object-detection",
        mode=WebRtcMode.SENDRECV,
        rtc_configuration={
            "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}],
        },
        video_frame_callback=video_frame_callback,
        media_stream_constraints={"video": True, "audio": False},
        async_processing=True,
    )

    if st.checkbox("Show the detected labels", value=True):
        if webrtc_ctx.state.playing:
            labels_placeholder = st.empty()
            # NOTE: The video transformation with object detection and
            # this loop displaying the result labels are running
            # in different threads asynchronously.
            # Then the rendered video frames and the labels displayed here
            # are not strictly synchronized.
            while True:
                result = result_queue.get()
                labels_placeholder.table(result)
